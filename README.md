# GENERATIVE-TEXT-MODEL

*COMPANY *: CODTECH IT SOLUTIONS

*NAME *: RIDHI SINGLA

*INTERN ID *: CT04DR441

*DOMAIN *: ARTIFICIAL INTELLIGENCE 

*DURATION *: 4 WEEEKS

*MENTOR *: NEELA SANTOSH

### ğŸ“‹ Project Description

This project is a **Generative Text Web App** built using **Flask** and **Ollama**.
It allows users to enter any topic or question, and the app generates **2â€“3 detailed paragraphs** of coherent, creative text using a **locally running AI model** (like **LLaMA 3**).

ğŸ’¡ The best part?
It works **entirely offline** â€” no cloud APIs, no internet connection, and no data sent outside your device.
Everything runs on your **local machine** via **Ollama**.

---

### ğŸ§° Tech Stack

* **Python 3.x**
* **Flask (Backend Web Framework)**
* **HTML/CSS (Frontend)**
* **Ollama (Local AI Model Runner)**
* **LLaMA 3 / Mistral / Gemma (Offline LLM Models)**

---

### âš™ï¸ Features

âœ… Generate meaningful, grammatically correct text for any topic
âœ… Output includes **2â€“3 detailed paragraphs (150â€“250 words)**
âœ… Fully **offline** once the model is downloaded
âœ… Clean, modern user interface
âœ… User input remains visible after generation
âœ… Simple and lightweight Flask app (no external dependencies)

---

### ğŸš€ How It Works

1. The user enters a **topic/question** in the text box.
2. Flask sends the prompt to the **local Ollama server** running at `http://localhost:11434`.
3. Ollama processes the request using your **locally installed LLM** (e.g., `llama3`).
4. The generated text is displayed instantly on the webpage.

No internet. No API keys. 100% local processing. ğŸ”’

---

### ğŸ§© Project Structure

```
â”œâ”€â”€ app.py                # Flask backend logic
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Frontend (HTML + CSS)
â””â”€â”€ README.md             # Project documentation
```

---

### âš¡ Installation & Setup

#### 1ï¸âƒ£ Install Ollama

Download and install Ollama from:
ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)

Once installed, open a terminal and pull your model (e.g., LLaMA 3):

```bash
ollama pull llama3
```

#### 2ï¸âƒ£ Install Dependencies

Create a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # on Windows: venv\Scripts\activate
```

Install Flask and Requests:

```bash
pip install flask requests
```

#### 3ï¸âƒ£ Start Ollama (runs locally)

```bash
ollama serve
```

This starts the local API server at `http://localhost:11434`.

#### 4ï¸âƒ£ Run the Flask App

In another terminal, run:

```bash
python app.py
```
Then open your browser and visit:

```
http://127.0.0.1:5000
```

### ğŸŒ Offline Functionality

This project works **100% offline** after setup:

* The AI model (LLaMA 3) runs locally through **Ollama**.
* Flask runs locally on your machine.
* All requests stay on `localhost` (no external calls or APIs).
* You can even **disable Wi-Fi** and the app will still function normally.

### ğŸ§‘â€ğŸ’» Example (OUTPUT)

